{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模块测试 梯度测试\n",
    "from turtle import forward\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "vgg = nn.Sequential(\n",
    "    nn.Conv2d(3, 3, (1, 1)),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(3, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU()  # relu5-4\n",
    ")\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    # assert (len(size) == 4)\n",
    "    other = size[:-2]\n",
    "    feat_var = feat.reshape(*other, -1).var(dim=-1) + eps\n",
    "    feat_std = feat_var.sqrt().reshape(*other, 1, 1)\n",
    "    feat_mean = feat.reshape(*other, -1).mean(dim=-1).reshape(*other, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "\n",
    "def adaIN(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:-2] == style_feat.size()[:-2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean) / content_std\n",
    "    return normalized_feat * style_std + style_mean\n",
    "\n",
    "def oldblock_adaIN(content_feat, style_feat, block = 16):\n",
    "    assert (content_feat.size()[:-2] == style_feat.size()[:-2])\n",
    "    assert content_feat.size()[-1] % block == 0\n",
    "    assert content_feat.size()[-2] % block == 0\n",
    "    size = content_feat.size()\n",
    "    N, C, H, W = size\n",
    "    newC = C * size[-1] * size[-2] / block / block\n",
    "    return adaIN(content_feat.reshape(N,C,H // block, block, W // block, block).transpose(3,4),\n",
    "                                        style_feat.reshape(N,C,H // block, block, W // block, block).transpose(3,4)\n",
    "                                        ).transpose(3,4).reshape(size)\n",
    "\n",
    "def block_adaIN(content_feat, style_feat, block = 16):\n",
    "    assert (content_feat.size()[:-2] == style_feat.size()[:-2])\n",
    "    content_feat = blockzation(content_feat, block)\n",
    "    style_feat = blockzation(style_feat, block)\n",
    "    return  unblockzation(adaIN(content_feat, style_feat))\n",
    "\n",
    "def blockzation(feat, block = 16):\n",
    "    H, W = feat.size()[-2:]\n",
    "    assert H % block == 0\n",
    "    assert W % block == 0\n",
    "    size = feat.size()[:-2]\n",
    "    feat = feat.reshape(*size, H // block, block, W // block, block).transpose(-2, -3)\n",
    "    return feat\n",
    "\n",
    "def unblockzation(feat):\n",
    "    size = feat.size()\n",
    "    H = size[-4] * size[-2]\n",
    "    W = size[-3] * size[-1]\n",
    "    size = size[:-4]\n",
    "    return feat.transpose(-2, -3).reshape(*size, H, W)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, level = 1):\n",
    "        super(Net, self).__init__()\n",
    "        enc_layers = list(encoder.children())[:44]\n",
    "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
    "        self.enc_5 = nn.Sequential(*enc_layers[31:44])  # relu4_1 -> relu5_1\n",
    "\n",
    "        self.level = level\n",
    "        # fix the encoder\n",
    "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4', 'enc_5']:\n",
    "            for param in getattr(self, name).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def encode_with_intermediate(self, input):\n",
    "        results = [input]\n",
    "        for i in range(4):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            results.append(func(results[-1]))\n",
    "        return results[1:]\n",
    "    \n",
    "    def encode_with_level(self, input, level):\n",
    "        for i in range(level):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            input = func(input)\n",
    "        return input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode_with_level(x, self.level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dynamic_adj_add(vec1, vec2):\n",
    "    assert vec1.shape == vec2.shape\n",
    "    shape = vec1.shape\n",
    "    vec1 = vec1.view(shape[0], -1)\n",
    "    vec2 = vec2.view(shape[0], -1)\n",
    "    v1v1 = (vec1 * vec1).mean(dim = 1)\n",
    "    v1v2 = (vec1 * vec2).mean(dim = 1)\n",
    "    v2v2 = (vec2 * vec2).mean(dim = 1)\n",
    "    gamma = min_norm_element_from2(v1v1, v1v2, v2v2).view(shape[0], 1)\n",
    "    return (gamma  * vec1 + (1 - gamma) * vec2).view(shape)\n",
    "\n",
    "def min_norm_element_from2(v1v1, v1v2, v2v2):\n",
    "        divide = v1v1+v2v2 - 2*v1v2\n",
    "        gamma = -1.0 * ( (v1v2 - v2v2) / (v1v1+v2v2 - 2*v1v2))\n",
    "        gamma = torch.where(torch.isnan(gamma), torch.full_like(gamma, 1), gamma)\n",
    "        return gamma.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(64,2)\n",
    "y = torch.ones(64,2)\n",
    "y[:,0] = .05\n",
    "\n",
    "print(dynamic_adj_add(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.1397e-05, grad_fn=<MeanBackward0>)\n",
      "tensor(20983.5996, grad_fn=<CopyBackwards>)\n",
      "tensor(528136.9375, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0032, grad_fn=<MeanBackward0>)\n",
      "tensor(21011.6348, grad_fn=<CopyBackwards>)\n",
      "tensor(532531.9375, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0068, grad_fn=<MeanBackward0>)\n",
      "tensor(21041.2070, grad_fn=<CopyBackwards>)\n",
      "tensor(538549.3750, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0105, grad_fn=<MeanBackward0>)\n",
      "tensor(21070.6367, grad_fn=<CopyBackwards>)\n",
      "tensor(545544.9375, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0144, grad_fn=<MeanBackward0>)\n",
      "tensor(21098.6582, grad_fn=<CopyBackwards>)\n",
      "tensor(552861.8125, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0185, grad_fn=<MeanBackward0>)\n",
      "tensor(21124.4336, grad_fn=<CopyBackwards>)\n",
      "tensor(560169.1250, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0228, grad_fn=<MeanBackward0>)\n",
      "tensor(21147.4727, grad_fn=<CopyBackwards>)\n",
      "tensor(567303.5000, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "tensor(21167.4727, grad_fn=<CopyBackwards>)\n",
      "tensor(574044.3750, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0316, grad_fn=<MeanBackward0>)\n",
      "tensor(21184.3340, grad_fn=<CopyBackwards>)\n",
      "tensor(580247.3125, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0362, grad_fn=<MeanBackward0>)\n",
      "tensor(21198.1680, grad_fn=<CopyBackwards>)\n",
      "tensor(585878.1875, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0408, grad_fn=<MeanBackward0>)\n",
      "tensor(21209.1406, grad_fn=<CopyBackwards>)\n",
      "tensor(590867.7500, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0455, grad_fn=<MeanBackward0>)\n",
      "tensor(21217.3887, grad_fn=<CopyBackwards>)\n",
      "tensor(595355.8125, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0502, grad_fn=<MeanBackward0>)\n",
      "tensor(21223.1836, grad_fn=<CopyBackwards>)\n",
      "tensor(599333.8125, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(21226.7266, grad_fn=<CopyBackwards>)\n",
      "tensor(602845.1875, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(21228.2734, grad_fn=<CopyBackwards>)\n",
      "tensor(605946.1250, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(21228.0137, grad_fn=<CopyBackwards>)\n",
      "tensor(608669.0625, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(21226.1641, grad_fn=<CopyBackwards>)\n",
      "tensor(611049.4375, grad_fn=<CopyBackwards>)\n",
      "tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(21222.8672, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sunsk/Projects/ilvr_adm/test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:23434/home/sunsk/Projects/ilvr_adm/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     blocked_x_in \u001b[39m=\u001b[39m normalized_blocked_x \u001b[39m*\u001b[39m std \u001b[39m+\u001b[39m mean\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:23434/home/sunsk/Projects/ilvr_adm/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://localhost:23434/home/sunsk/Projects/ilvr_adm/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(loss\u001b[39m.\u001b[39;49msum(), blocked_x_in)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:23434/home/sunsk/Projects/ilvr_adm/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     blocked_x_in \u001b[39m=\u001b[39m blocked_x_in \u001b[39m-\u001b[39m iterate_lr \u001b[39m*\u001b[39m grad\n\u001b[1;32m     <a href='vscode-notebook-cell://localhost:23434/home/sunsk/Projects/ilvr_adm/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m x_feat \u001b[39m=\u001b[39m net(unblockzation(blocked_x_in))\n",
      "File \u001b[0;32m~/anaconda3/envs/sde/lib/python3.8/site-packages/torch/autograd/__init__.py:226\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 226\u001b[0m \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    227\u001b[0m     outputs, grad_outputs_, retain_graph, create_graph,\n\u001b[1;32m    228\u001b[0m     inputs, allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 待测试方案\n",
    "# 以下每行的排列组合\n",
    "##  是否使用blockadain初始化\n",
    "##  MSE or KL \n",
    "##  仅仅调节std和mean (blockadain level shape (batch, channel, area, area, 1, 1))还是直接调节feature map\n",
    "###  MSE with std&mean 是直接std和mean分别的MSE\n",
    "##  迭代次数 (考虑是用adam之类的优化器)\n",
    "##  迭代的学习率\n",
    "##  差分值的系数 \n",
    "##  vgg网络深度\n",
    "\n",
    "# 一个 optimization\n",
    "# 此外还需要实现加噪信号的block 均值和方差的计算(推一推公式)\n",
    "# basic\n",
    "# x_t = a * x + b * z\n",
    "# E(x_t) = a * E(x)\n",
    "# Var(x_t) = a^2 * var(x) + b^2 \n",
    "# 计算好ref image的block level的均值和方差之后, 可以通过计算的方式直接得到需要的均值和方差\n",
    "\n",
    "####### things to adjust\n",
    "\n",
    "### 梯度方案很容易偏色\n",
    "### 仅仅迁移均值方差细节迁移不到位\n",
    "### 两种方案合在一起, 先做neural feature的细节transfer, 再做image level的blockAdain\n",
    "init_with_blockadain = True\n",
    "# MSE is the MSE with adained feature map, While KL is the KL divergence of two distribulation, \n",
    "# but it has the same best optimization point with L2 distance between means, and between vars, further try to use multi task opt methods\n",
    "losstype = \"KL\" # \"MSE\" or \"KL\"\n",
    "# when use affine, the image level features only changes as affine\n",
    "affine = False # keep image features affine\n",
    "iterate_times = 30000 # the iterate times\n",
    "iterate_lr = 1e-1 / 64 # the iterate learning rate \n",
    "difference_std = 7 # the std of difference / the std of the score\n",
    "vggdeep = 1 # the feature extractor depth 1/2/3\n",
    "area = 16 # in 1d how many part to divide 8/16/32\n",
    "\n",
    "vgg.load_state_dict(torch.load(\"/home/sunsk/Models/vgg/vgg_normalised.pth\"))\n",
    "\n",
    "net = Net(vgg, level = vggdeep)\n",
    "net.eval()\n",
    "x = torch.randn(64,3,256,256) # source \n",
    "y = torch.randn(64,3,256,256)*3 + 5 # target features\n",
    "H, W = x.shape[-2:]\n",
    "blocked_x = blockzation(x, block = H // area)\n",
    "blocked_y = blockzation(y, block= H // area)\n",
    "# prepare features to optimization\n",
    "with torch.enable_grad():\n",
    "    if affine:\n",
    "        blocked_x_mean, blocked_x_std = calc_mean_std(blocked_x) # [batchsize, channel, area, area, 1, 1]\n",
    "        blocked_y_mean, blocked_y_std = calc_mean_std(blocked_y) # can be caltulated before\n",
    "        if init_with_blockadain:\n",
    "            mean = blocked_y_mean.detach().requires_grad_(True) # need to be updated to block level\n",
    "            std = blocked_y_std.detach().requires_grad_(True)\n",
    "        else:\n",
    "            mean = blocked_x_mean.detach().requires_grad_(True)\n",
    "            std = blocked_x_std.detach().requires_grad_(True)\n",
    "        \n",
    "        normalized_blocked_x = (blocked_x - blocked_x_mean) / blocked_x_std\n",
    "        blocked_x_in = normalized_blocked_x * std + mean\n",
    "    else:\n",
    "        if init_with_blockadain:\n",
    "            blocked_x_in = adaIN(blocked_x, blocked_y).detach().requires_grad_(True)\n",
    "        else:\n",
    "            blocked_x_in = blocked_x.detach().requires_grad_(True)\n",
    "\n",
    "    x_feat = net(unblockzation(blocked_x_in))\n",
    "    y_feat = net(y)\n",
    "    H, W = x_feat.shape[-2:]\n",
    "    blocked_x_feat = blockzation(x_feat,block = H // area)\n",
    "    blocked_y_feat = blockzation(y_feat,block = H // area)\n",
    "\n",
    "    blocked_y_feat_mean, blocked_y_feat_std = calc_mean_std(blocked_y_feat)\n",
    "\n",
    "    for _ in range(iterate_times):\n",
    "        print(blocked_x_in.mean())\n",
    "        print((blocked_x_in - blocked_y).norm())\n",
    "        if losstype == \"MSE\":\n",
    "            blocked_target = adaIN(blocked_x_feat, blocked_y_feat)\n",
    "            loss = (blocked_x_feat - blocked_target) ** 2\n",
    "        # elif losstype == \"KL\":\n",
    "        else:\n",
    "            blocked_x_feat_mean, blocked_x_feat_std = calc_mean_std(blocked_x_feat)\n",
    "            # original but not stable loss\n",
    "            # loss = x_var / y_var + (x_mean - y_mean) / y_var - torch.log(x_var / y_var) - 1\n",
    "            # stable loss\n",
    "            loss = (blocked_x_feat_mean - blocked_y_feat_mean) ** 2 + (blocked_x_feat_std - blocked_y_feat_std) ** 2\n",
    "\n",
    "        if affine:\n",
    "            grad1 = torch.autograd.grad(loss.sum(), std, retain_graph= True)[0]\n",
    "            grad2 = torch.autograd.grad(loss.sum(), mean)[0]\n",
    "            std = std - iterate_lr * grad1\n",
    "            mean = mean - iterate_lr * grad2\n",
    "            blocked_x_in = normalized_blocked_x * std + mean\n",
    "        else:\n",
    "            grad = torch.autograd.grad(loss.sum(), blocked_x_in)[0]\n",
    "            blocked_x_in = blocked_x_in - iterate_lr * grad\n",
    "        x_feat = net(unblockzation(blocked_x_in))\n",
    "        blocked_x_feat = blockzation(x_feat,block = H // area)\n",
    "        print((blocked_x_feat - blocked_y_feat).norm())\n",
    "        # target = block_adaIN(x_feat, y_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 64, 64)\n",
    "y = torch.ones(2, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(block_adaIN(x, y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n"
     ]
    }
   ],
   "source": [
    "### grad 测试\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "\n",
    "func = models.resnet101(pretrained=True)\n",
    "data = torch.rand(64,3,64,64)\n",
    "func.eval()\n",
    "print(func(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(811.0899)\n",
      "811.0912\n"
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "    x_in = data.detach().requires_grad_(True)\n",
    "    logits = func(x_in).view(64,-1).sum(dim=-1)\n",
    "    print(torch.autograd.grad(logits.sum(), x_in)[0].norm())\n",
    "    print(64*12.6733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "def dynamic_adj_add(vec1, vec2):\n",
    "    # print(vec1.shape)\n",
    "    assert vec1.shape == vec2.shape\n",
    "    shape = vec1.shape\n",
    "    vec1 = vec1.view(shape[0], -1)\n",
    "    vec2 = vec2.view(shape[0], -1)\n",
    "    v1v1 = (vec1 * vec1).mean(dim = 1)\n",
    "    v1v2 = (vec1 * vec2).mean(dim = 1)\n",
    "    v2v2 = (vec2 * vec2).mean(dim = 1)\n",
    "    gamma = min_norm_element_from2(v1v1, v1v2, v2v2).view(shape[0], 1)\n",
    "    coef = ((1 - gamma)/gamma).clamp(0,30)\n",
    "    coef = th.where(th.isnan(coef), th.full_like(coef, 0), coef)\n",
    "    # return (gamma * vec1 + (1 - gamma) * vec2).view(shape)\n",
    "    return (vec1 + coef * vec2).view(shape)\n",
    "\n",
    "def min_norm_element_from2(v1v1, v1v2, v2v2):\n",
    "    divide = v1v1+v2v2 - 2*v1v2\n",
    "    gamma = (v2v2 - v1v2) / divide\n",
    "    gamma = th.where(th.isnan(gamma), th.full_like(gamma, 0), gamma)\n",
    "    return gamma.clamp(0, 1)\n",
    "\n",
    "# veclist shape: [batch, number, others]\n",
    "# 方案一, 提取跟diffuison 方向垂直的分量, 将这些分量想办法dynamic化\n",
    "# 方案二, 将当前的方案暴力拓展到三个变量的情况\n",
    "def frank_wolfe_solver(veclist, ep = 1e-4, maxnum = 20):\n",
    "    shape = veclist.shape\n",
    "    veclist = veclist.view(shape[0], shape[1], -1) # shape [B, N, O]\n",
    "    M = veclist @ veclist.transpose(1,2) # shape [B, N, N]\n",
    "    a = (th.ones(shape[:2]) / shape[1]).unsqueeze(1).to(veclist.device) # shape [B, 1, N]\n",
    "    for _ in range(maxnum):\n",
    "        minrank = th.argmin(a @ M, dim = 2) # shape [B, 1]\n",
    "        minonehot = th.zeros(shape[:2]).to(veclist.device).scatter_(1, minrank, 1).unsqueeze(1) # shape [B, 1, N]\n",
    "        gamma = min_norm_element_from2(minonehot @ M @ minonehot.transpose(1,2),minonehot @ M @ a.transpose(1,2), a @ M @ a.transpose(1,2)).reshape(-1, 1, 1)\n",
    "        # minvec = th.diagonal(veclist[:,minrank]).transpose(0,1)\n",
    "        a = (1-gamma)* a + gamma * minonehot\n",
    "        if th.abs(gamma).mean()< ep:\n",
    "            return a\n",
    "    return a\n",
    "\n",
    "veclist = th.rand(3,3,3,64,64).to(device=\"cuda\")\n",
    "a = frank_wolfe_solver(veclist)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([64, 512, 32, 32])\n",
      "tensor(48.5318, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "path = \"/home/sunsk/Models/resnet50/resnet50-19c8e357.pth\"\n",
    "resnet50 = models.resnet50(pretrained=False)\n",
    "resnet50.load_state_dict(torch.load(path))\n",
    "resnet50.eval()\n",
    "print(len(list(resnet50.children())))\n",
    "newmodel = torch.nn.Sequential(*(list(resnet50.children())[:6]))\n",
    "cos = torch.nn.CosineSimilarity(eps = 1e-6)\n",
    "batch = x.shape[0]\n",
    "x = torch.rand(64,3,256,256)\n",
    "y = torch.rand(64,3,256,256)\n",
    "xh = newmodel(x)\n",
    "yh = newmodel(y)\n",
    "cossimi = cos(xh, yh).mean() * batch\n",
    "print(xh.shape)\n",
    "print(cossimi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7101],\n",
      "          [0.0259],\n",
      "          [0.5566]]]])\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.9167]]]])\n",
      "torch.Size([1, 1, 3, 1])\n",
      "1 1 3 1\n",
      "tensor([[[[0.7101],\n",
      "          [0.0259],\n",
      "          [0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(1, 1, 3, 1)\n",
    "y = torch.rand(1, 1, 3, 1)\n",
    "y[:,:,:2,:] = 0\n",
    "print(x)\n",
    "print(y)\n",
    "print(get_vertical_component(x, y, independdims=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sde')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18f6dd5e28ea51b7c193753a74cd2f37f776e591a014247ffd6312f5124d577c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
